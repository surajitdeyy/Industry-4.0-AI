{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import *\n",
    "\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "from fastai.metrics import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from collections import defaultdict, namedtuple\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../data/plant-seedlings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch=resnet50\n",
    "sz = 224\n",
    "bs = 64\n",
    "\n",
    "tfms = tfms_from_model(arch, sz, aug_tfms=transforms_top_down, max_zoom=1.2)\n",
    "data = ImageClassifierData.from_paths(path, tfms=tfms, test_name='test', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(load_img_id(data.val_ds, 1, path))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.trn_ds.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "num_train = 0\n",
    "label_counts = Counter(y).most_common()\n",
    "for l, c in label_counts:\n",
    "    num_train = num_train + c\n",
    "    print(c, '\\t', data.classes[l])\n",
    "    \n",
    "print(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in sorted([l for l, c in label_counts], key=lambda p: data.classes[p]):\n",
    "    i = [i for i, l in enumerate(y) if l == label][0]\n",
    "    print(data.classes[y[i]])\n",
    "    plt.imshow(load_img_id(data.trn_ds, i, path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "\n",
    "folder = path + '/all/Black-grass'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "sizes = []\n",
    "\n",
    "for file in files:\n",
    "    filename = os.path.join(folder, file)\n",
    "    img = cv2.imread(filename, flags)\n",
    "    sizes.append(max(img.shape[0], img.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sizes, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image lighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try histogram equalization to improve constrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "\n",
    "folder = path + '/all/Black-grass'\n",
    "\n",
    "files = os.listdir(folder)\n",
    "for i in range(5):\n",
    "    f = plt.figure(figsize=(5, 5))\n",
    "    \n",
    "    filename = os.path.join(folder, files[i])\n",
    "    img = cv2.imread(filename, flags)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    sp = f.add_subplot(1, 2, 1)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    img = np.concatenate([np.expand_dims(cv2.equalizeHist(img[:,:,i]), axis=2) for i in range(3)], axis=2)\n",
    "    \n",
    "    \n",
    "    sp = f.add_subplot(1, 2, 2)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal version\n",
    "\n",
    "def open_image_normal(fn):\n",
    "    \"\"\" Opens an image using OpenCV given the file path.\n",
    "\n",
    "    Arguments:\n",
    "        fn: the file path of the image\n",
    "\n",
    "    Returns:\n",
    "        The numpy array representation of the image in the RGB format\n",
    "    \"\"\"\n",
    "    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "    if not os.path.exists(fn):\n",
    "        raise OSError('No such file or directory: {}'.format(fn))\n",
    "    elif os.path.isdir(fn):\n",
    "        raise OSError('Is a directory: {}'.format(fn))\n",
    "    else:\n",
    "        try:\n",
    "            return cv2.cvtColor(cv2.imread(fn, flags), cv2.COLOR_BGR2RGB).astype(np.float32)/255\n",
    "        except Exception as e:\n",
    "            raise OSError('Error handling image at: {}'.format(fn)) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram equalization\n",
    "\n",
    "def open_image_hist_eq(fn):\n",
    "    \"\"\" Opens an image using OpenCV given the file path.\n",
    "\n",
    "    Arguments:\n",
    "        fn: the file path of the image\n",
    "\n",
    "    Returns:\n",
    "        The numpy array representation of the image in the RGB format\n",
    "    \"\"\"\n",
    "    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR\n",
    "    if not os.path.exists(fn):\n",
    "        raise OSError('No such file or directory: {}'.format(fn))\n",
    "    elif os.path.isdir(fn):\n",
    "        raise OSError('Is a directory: {}'.format(fn))\n",
    "    else:\n",
    "        try:\n",
    "            img = cv2.cvtColor(cv2.imread(fn, flags), cv2.COLOR_BGR2RGB)\n",
    "            img = np.concatenate([np.expand_dims(cv2.equalizeHist(img[:,:,i]), axis=2) for i in range(3)], axis=2)\n",
    "            return img.astype(np.float32)/255\n",
    "        except Exception as e:\n",
    "            raise OSError('Error handling image at: {}'.format(fn)) from e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the 2nd line below to apply histogram equalization to fastai dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open_image = open_image_normal\n",
    "#open_image = open_image_hist_eq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can change image augmentation parameters and see how augmented images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at examples of image augmentation\n",
    "def get_augs():\n",
    "    x,_ = next(iter(data.aug_dl))\n",
    "    return data.trn_ds.denorm(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "#aug_tfms = [RandomRotate(20), RandomLighting(0.8, 0.8)]\n",
    "tfms = tfms_from_model(arch, sz, aug_tfms=transforms_top_down, max_zoom=1.2)\n",
    "data = ImageClassifierData.from_paths(path, tfms=tfms, test_name='test', bs=bs)\n",
    "\n",
    "ims = np.stack([get_augs() for i in range(6)])\n",
    "plots(ims, rows=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a learner\n",
    "learn = ConvLearner.pretrained(arch, data, precompute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for a good starting learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_change(sched, sma=1, n_skip=20, y_lim=(-0.01,0.01)):\n",
    "    \"\"\"\n",
    "    Plots rate of change of the loss function.\n",
    "    Parameters:\n",
    "        sched - learning rate scheduler, an instance of LR_Finder class.\n",
    "        sma - number of batches for simple moving average to smooth out the curve.\n",
    "        n_skip - number of batches to skip on the left.\n",
    "        y_lim - limits for the y axis.\n",
    "    \"\"\"\n",
    "    derivatives = [0] * (sma + 1)\n",
    "    for i in range(1 + sma, len(learn.sched.lrs)):\n",
    "        derivative = (learn.sched.losses[i] - learn.sched.losses[i - sma]) / sma\n",
    "        derivatives.append(derivative)\n",
    "        \n",
    "    plt.ylabel(\"d/loss\")\n",
    "    plt.xlabel(\"learning rate (log scale)\")\n",
    "    plt.plot(learn.sched.lrs[n_skip:], derivatives[n_skip:])\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_change(learn.sched, sma=20, n_skip=20, y_lim=(-0.03, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with LR 0.01 for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(0.01, 20, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('step1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(0.01, 2, cycle_len=1, cycle_mult=2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('step1_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('step1_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_change(learn.sched, sma=20, n_skip=20, y_lim=(-0.01, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unfreeze and train with LR 0.01 for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('unfreeze1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(1e-3, 1, wds=wd)\n",
    "#learn.fit(0.01, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('unfreeze1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_change(learn.sched, sma=20, n_skip=20, y_lim=(-0.001, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for a few cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('unfreeze1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr, 1, cycle_len=1, cycle_mult=2, wds=wd)\n",
    "#learn.fit(lr, 4, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_loss_change(learn.sched, sma=20, n_skip=20, y_lim=(-0.02, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_preds,y = learn.predict_with_targs()\n",
    "preds = np.exp(log_preds)\n",
    "pred_labels = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, pred_labels)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ImageModelResults(data.val_ds, log_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = [i for i in range(len(pred_labels)) if pred_labels[i] != y[i]]\n",
    "c = Counter([(y[i], data.classes[y[i]]) for i in incorrect])\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results.plot_most_incorrect(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_most_incorrect(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot_most_incorrect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_most_correct(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_most_correct(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results.plot_most_correct(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most uncertain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot_most_uncertain(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test time augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds,y = learn.TTA(n_aug=20)\n",
    "preds = np.mean(np.exp(log_preds),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_np(preds, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain on the training set + validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation loss/accuracy won't be indicative of the model performance because the validation set is a subset of the training set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet50\n",
    "sz = 224\n",
    "bs = 64\n",
    "wd = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aug_tfms = [RandomRotate(20), RandomLighting(0.8, 0.8)]\n",
    "tfms = tfms_from_model(arch, sz, aug_tfms=transforms_top_down, max_zoom=1.2)\n",
    "data = ImageClassifierData.from_paths(path, tfms=tfms, trn_name='all', val_name='valid', test_name='test', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data, precompute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 10, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('step1_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('step1_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 2, cycle_len=1, cycle_mult=2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('step1_cycle_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('step1_cycle_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('final_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit(1e-3, 1, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('final_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('final_full_val013_997')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('final_full_val013_997')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('final_full_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1e-3, 1, cycle_len=1, cycle_mult=2, wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('final_full_cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds, y = learn.TTA(n_aug=20) # (5, 2044, 120), (2044,)\n",
    "probs = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(probs, y), metrics.log_loss(y, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(probs, axis=1)\n",
    "print(probs.shape)\n",
    "print(preds)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y, preds)\n",
    "plot_confusion_matrix(cm, data.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_log_predictions, _ = learn.TTA(is_test=True)\n",
    "test_probs = np.mean(np.exp(test_log_predictions),0)\n",
    "test_predictions = np.argmax(test_probs, axis=1)\n",
    "print(test_predictions.shape)\n",
    "test_predictions_classes = [data.classes[pred] for pred in test_predictions]\n",
    "test_file_names = learn.data.test_ds.fnames\n",
    "\n",
    "with open('submission.csv', 'w') as the_file:\n",
    "    the_file.write('file,species\\n')\n",
    "    for file_path, prediction in zip(test_file_names, test_predictions_classes):\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        the_file.write(file_name)\n",
    "        the_file.write(\",\")\n",
    "        the_file.write(prediction)\n",
    "        the_file.write(\"\\n\")\n",
    "the_file.close()\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def winner(input): \n",
    "  \n",
    "     # convert list of candidates into dictionary \n",
    "     # output will be likes candidates = {'A':2, 'B':4} \n",
    "     votes = Counter(input)\n",
    "     #print(votes)\n",
    "       \n",
    "     # create another dictionary and it's key will \n",
    "     # be count of votes values will be name of  \n",
    "     # candidates \n",
    "     dict = {} \n",
    "  \n",
    "     for value in votes.values(): \n",
    "  \n",
    "          # initialize empty list to each key to  \n",
    "          # insert candidate names having same  \n",
    "          # number of votes  \n",
    "          dict[value] = [] \n",
    "  \n",
    "     for (key,value) in votes.items(): \n",
    "          dict[value].append(key) \n",
    "  \n",
    "     # sort keys in descending order to get maximum  \n",
    "     # value of votes \n",
    "     maxVote = sorted(dict.keys(),reverse=True)[0] \n",
    "  \n",
    "     # check if more than 1 candidates have same  \n",
    "     # number of votes. If yes, then sort the list \n",
    "     # first and print first element \n",
    "     if len(dict[maxVote])>1:\n",
    "         return sorted(dict[maxVote])[0]\n",
    "     else:\n",
    "         return dict[maxVote][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submission1 = './submission23.csv'\n",
    "submission2 = './submission22.csv'\n",
    "submission3 = './submission21.csv'\n",
    "submission4 = './submission18.csv'\n",
    "submission5 = './submission17.csv'\n",
    "submission6 = './submission17.csv'\n",
    "submission7 = './submission16.csv'\n",
    "\n",
    "counter = 0\n",
    "\n",
    "df1 = pd.read_csv(submission1)\n",
    "df2 = pd.read_csv(submission2)\n",
    "df3 = pd.read_csv(submission3)\n",
    "df4 = pd.read_csv(submission4)\n",
    "df5 = pd.read_csv(submission5)\n",
    "df6 = pd.read_csv(submission6)\n",
    "df7 = pd.read_csv(submission7)\n",
    "\n",
    "classes = []\n",
    "speciesList = []\n",
    "\n",
    "for i in range(0, len(df1)):\n",
    "    classes.append(winner([df1['species'][i], df2['species'][i], df3['species'][i], df4['species'][i], df5['species'][i], df6['species'][i], df7['species'][i]]))\n",
    "\n",
    "with open('submission.csv', 'w') as the_file:\n",
    "    the_file.write('file,species\\n')\n",
    "    for file_path, prediction in zip(test_file_names, classes):\n",
    "        file_name = file_path.split('/')[-1]\n",
    "        the_file.write(file_name)\n",
    "        the_file.write(\",\")\n",
    "        the_file.write(prediction)\n",
    "        the_file.write(\"\\n\")\n",
    "the_file.close()\n",
    "\n",
    "from IPython.display import FileLink\n",
    "FileLink('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_test_labels = {a.filename: a.label for a in test_annotations}\n",
    "class_indexes = {c: i for i, c in enumerate(data.classes)}\n",
    "filenames = [filepath[filepath.find('/') + 1:] for filepath in data.test_ds.fnames]\n",
    "labels = [str(true_test_labels[filename]) for filename in filenames]\n",
    "y_true = np.array([class_indexes[label] for label in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds = learn.predict(is_test=True)\n",
    "preds = np.exp(log_preds)\n",
    "accuracy_np(preds, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds,_ = learn.TTA(n_aug=20, is_test=True)\n",
    "preds = np.mean(np.exp(log_preds),0)\n",
    "accuracy_np(preds, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = [i for i in range(len(pred_labels)) if pred_labels[i] != y_true[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(class_names[data.classes[y_true[incorrect[i]]]], class_names[data.classes[pred_labels[incorrect[i]]]], \n",
    "          preds[incorrect[i], y_true[incorrect[i]]], preds[incorrect[i], pred_labels[incorrect[i]]])\n",
    "    plt.imshow(load_img_id(data.test_ds, incorrect[i], path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join(path, 'confusion_matrix.tsv'), cm, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = Counter([class_names[data.classes[y_true[i]]] for i in incorrect])\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter([class_names[data.classes[pred_labels[i]]] for i in incorrect])\n",
    "c.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(preds, axis=1)\n",
    "pred_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    class_id = data.classes[pred_labels[i]]\n",
    "    filename = data.test_ds.fnames[i].split('/')[1]\n",
    "    print(filename, class_id, class_names[class_id])\n",
    "    plt.imshow(load_img_id(data.test_ds, i, path))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/submission.csv', 'w') as f:\n",
    "    for i in range(pred_labels.shape[0]):\n",
    "        filename = data.test_ds.fnames[i].split('/')[1]\n",
    "        f.write('{};{}\\n'.format(filename, data.classes[pred_labels[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('data/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
